HADOOP2
=============================
/home/hadoop/hadoop/etc/hadoop ----All configuration files are there.
/home/hadoop/hadoop/bin
/home/hadoop/hadoop/sbin ---> all admin commnads are placed (service)
/home/hadoop/hadoop/share/hadoop ---> all jar files are placed here.

set the path in bash profile.
============================================================================
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export JAVA_HOME=/usr/java/jdk1.8.0_144
PATH=$HADOOP_HOME/bin/:$HADOOP_HOME/sbin/:$JAVA_HOME/bin/:$PATH
export PATH
=============================================================================
Namenode configuration:-
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
------------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/data/namenode</value>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

hadoop namenode -format
hadoop-daemon.sh start namenode
============================================================================
Datanode Configuration:-
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
--------------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/data/datanode</value>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

hadoop-daemon.sh start namenode

hdfs dfsadmin -report
==============================================================================
configure Jobtracker/Resource manager
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
---------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
----------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>jt.example.com:9001</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>jt.example.com:9002</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>jt.example.com:9003</value>
</property>
</configuration>

hadoop-daemon.sh start resourcemanager --- not work.
yarn-daemon.sh start resourcemanager
-------------------------------------------------------------------------------------
configuration nodemanager in datanodes.
copy all yarn-site.xml and mapred-site.xml configure in all datanodes and start nodemanager in datanode
yarn-daemon.sh start nodemanager
=======================================================================
 for i in 192.168.9.{101,102,103}; do ssh $i '/usr/java/jdk1.8.0_144/bin/jps; hostname'; echo " "; done;
