HADOOP2
=============================
/home/hadoop/hadoop/etc/hadoop ----All configuration files are there.
/home/hadoop/hadoop/bin
/home/hadoop/hadoop/sbin ---> all admin commnads are placed (service)
/home/hadoop/hadoop/share/hadoop ---> all jar files are placed here.

set the path in bash profile.
============================================================================
export HADOOP_HOME=/home/hadoop/hadoop
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export JAVA_HOME=/usr/java/jdk1.8.0_144
PATH=$HADOOP_HOME/bin/:$HADOOP_HOME/sbin/:$JAVA_HOME/bin/:$PATH
export PATH
=============================================================================
Namenode configuration:-
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
------------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/data/namenode</value>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

hadoop namenode -format
hadoop-daemon.sh start namenode
============================================================================
Datanode Configuration:-
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
--------------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/data/datanode</value>
</property>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>

hadoop-daemon.sh start namenode

hdfs dfsadmin -report
==============================================================================
configure Jobtracker/Resource manager
vi /home/hadoop/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn1.example.com:9000</value>
</property>
</configuration>
---------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
----------------------------------------------------------------------------
vi /home/hadoop/hadoop/etc/hadoop/yarn-site.xml
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>jt.example.com:9001</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>jt.example.com:9002</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>jt.example.com:9003</value>
</property>
</configuration>

hadoop-daemon.sh start resourcemanager --- not work.
yarn-daemon.sh start resourcemanager
-------------------------------------------------------------------------------------
configuration nodemanager in datanodes.
copy all yarn-site.xml and mapred-site.xml configure in all datanodes and start nodemanager in datanode
yarn-daemon.sh start nodemanager
=======================================================================
 for i in 192.168.9.{101,102,103}; do ssh $i '/usr/java/jdk1.8.0_144/bin/jps; hostname'; echo " "; done;
 for i in 192.168.9.{101,102,103}; do ssh $i '/usr/java/jdk1.8.0_144/bin/jps; hostname'; echo " "; done;
 
yarn jar hadoop/share/mapreduce/hadoop-mapreduce-examples-2.4.0.jar wordcount /input /output
for i in 192.168.9.{101,102,103}; do ssh $i 'hostname; /usr/java/jdk1.8.0_144/bin/jps;echo " "; done; 

yarn jar hadoop/share/mapreduce/hadoop-mapreduce-examples-2.4.0.jar pai 4 6
========================================================
http://blog.csdn.net/asd315861547/article/details/56276945

hive metastore in durby but it not in production we use mysql.
hadoopp and hive application connect through some connector called hcatlog.

download the hive extract it (client machine) add/set path in .bash_profile
#vi hive/conf/hive-default.xml  (copy or cnage the name of hive-default-templete file)
first create location where hive will create our metadata.
#hadoop fs -ls /
#hadoop fs -mkdir -p /user/hive/warehouse
#vi hive-default.xml
<name>hive.metastore.warehouse.dir</name>
<value>/user/hive/warehouse</value>
change the ownership of above directory.
#hadoop fs -chmod g_w /user/hive/warehouse (group--write permission)
write some data:-

hadoop fs -ls /user/hive/warehouse

run hive. (sometime it showing this error:- Exception in thread "main" java.lang.RuntimeException: 
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: 
Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient)

delete metastore_db directroy and derby.log.

[hadoop@client1 ~]$ rm -rf metastore_db/
[hadoop@client1 ~]$ rm -rf derby.log

[hadoop@client1 ~]$ schematool -dbType derby -initSchema    ( run this command)
which: no hbase in (/home/hadoop/hive/bin:/home/hadoop/hadoop/bin/:/home/hadoop/hadoop/sbin/:/usr/java/jdk1.8.0_144/bin/:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)
Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true
Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver
Metastore connection User:       APP
Starting metastore schema initialization to 2.1.0
Initialization script hive-schema-2.1.0.derby.sql
Initialization script completed
schemaTool completed
[hadoop@client1 ~]$ hive     ( run again hive command)
which: no hbase in (/home/hadoop/hive/bin:/home/hadoop/hadoop/bin/:/home/hadoop/hadoop/sbin/:/usr/java/jdk1.8.0_144/bin/:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/hadoop/.local/bin:/home/hadoop/bin)

Logging initialized using configuration in jar:file:/home/hadoop/hive/lib/hive-common-2.1.1.jar!/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> [hadoop@client1 ~]$

hive>show databases
create database test;
use test;
CREATE TABLE POKES (foo INT, bar STRING);
LOAD DATA LOCAL INPATH '.hive/examples/kv1.txt' OVERWRITE INTO TABLE pokes;
select * from pokes;
quit





 
