hadoop fs -put test     (copy file to hadoop cluster)
hadoop fs -ls /			(Listing file from hadoop cluster)
hadoop fs -mkdir /projects  (creating directory in hadoop cluster)
hadoop fsck /test -files -blocks -locations  ( checking files locations)
hadoop fsck /rack -files -blocks -locations -racks  (checking files locations in racks)
hadoop fs -copyToLocal /list_of_datanodes .   (copy file from hadoop to local disk)
hadoop dfsadmin -safemode get   ( check namenode status)
hadoop dfsadmin -safemode enter ( enter the namenode in safemode (safemode on)
hadoop dfsadmin -safemode leave (leave the safemode)
hadoop fs -Ddfs.replication=2 -put largefile.txt /projects
hadoop dfs -setrep 2 /test   (Now set replication 2 for existing file)
systemctl list-unit-files| less   ( check all services)
systemctl disable firewalld.service ( disable the firewall service)
----------------------------------------------------------------------------------------------
# hadoop balancer -threshold 10 (precentage if any node greter then 10% of my whole cluster) 
 first start balancer daemons.
# start-balancer.sh ( start balancer daemons.)
hadoop dfsadmin -setBalancerBandwidth 5000 (bytes in per second) 
===============================================================================
Creating Quota:-
hadoop fs -mkdir /projects
hadoop fs -mkdir /projects/marketing
hadoop fs -mkdir /projects/finance
hadooop fs -count -q /  (check the quota)
hadoop fs -count /projects
hadoop fs -count -q /projects
hadoop dfsadmin -setQuota 10 /projects   (set the file creation quota)
hadoop dfsadmin -setSpaceQuota 200M /projects  (set the space quota)
hadoop fs -count -q /projects
Without increasing quota we can change the blocksize (in command line) and copied file to hadoop
hadoop fs -Ddfs.replication=2 -Ddfs.block.size=100663296 -put largefile.txt /projects
hadoop fs -Ddfs.replication=1 -Ddfs.block.size=201326592 -put largefile.txt /projects
dd if=/dev/zero of=large.txt bs=1024 count=1000 (creating 1 mb file)
hadoop dfsadmin -setSpaceQuota 200M /projects  (set the space quota)
hadoop fs -count -q /projects

Clear the quota
hadoop dfsadmin -clrSpaceQuota /projects
hadoop dfsadmin -clrQuota /projects
=================================================================================================
Backup:-
need to be online jobtracker ( start-mapred.sh)
distcp command to copy /projects to /new on the same cluster
hadoop distcp hdfs://nn1.example.com:9000/ hdfs://nn1.example.com:9000/newdata
==============================================================================================-

hadoop balancer
hadoop balancer -threshold 10 (10 is by default and small cluster have more value compare then large cluster)
start-balancer.sh
hadoop dfsadmin -setBalancerBandwidth 5000 (bytes in per second)
example:-
N= number of node
S= space size (storage)
Total=NS
it means 10 times of node=30% if we have 3 node and each have 12GB
================================
hadoop dfsadmin -refreshNodes
hadoop fs -arc file1 file2 file3 -put /
=============================================

manually checkpointing.

first we need to be namenmode in safemode.
hadoop dfsadmin -safemode get   ( check namenode status)
hadoop dfsadmin -safemode enter ( enter the namenode in safemode (safemode on)
hadoop dfsadmin -saveNamespace
hadoop dfsadmin -safemode leave (leave the safemode)
hadoop dfsadmin -safemode wait ( it will give 30 second extra time to datanode to contact NN)
====================================================================
Recycle bin

recycle will configure in namenmode:- core-site.xml file.

<name> fs.trash.interval</name>
<value>40</value>
create a file and delete it should be go in trash. one user directory will created
hadoop fs -ls /user/hadoop/.Trash/Current/........
#hadoop dfs -expunge (empty the recycle bin)
=====================================================================================
SNN (secondary namenode)
it will do checkpointing without any effecting to primary namenode.
configure SNN.
#vi hadoop/conf/core-site.xml  (put namenode entry)
#vi hdfs-site.xml
put entry of SNN
start SNN
# hadoop secondarynamenode -checkpoint force
=============================================
Decommission and commission the nodes
 NN:- hdfs-site.xml
 Jt:- mapred-site.xml
 hadoop dfsadmin refreshNodes
 hadoop mradmin refreshNodes
 =======================================================
 user access hadoop cluster 
 adduser:- created/password user in all nodes
for i in 192.168.9.{100,101,102,103,201}; do ssh $i 'useradd dinesh'; done;
for i in 192.168.9.{100,101,102,103,201}; do ssh $i 'passwd  dinesh'; done;
hadoop fs chown manu:manu /manu
hadoop fs chmod 700 /manu
usermod -G hadoop dinesh
set the profile---
create home dir for all user in hadoop filesystem and change the ownership.
hadoop fs -chown dinesh:dinesh /user/dinesh
check in JT aslo (change the /tmp directory permission 777)
